{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lined-enlargement",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue;\">**Asthma Detection from human breathing**</span>\n",
    "\n",
    "## <span style=\"color:green;\">**Course**</span>: Topological Data Analysis  \n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:orange;\">**Group Members:**</span>\n",
    "- **Aubrey UNDI PHIRI**  \n",  
    "- **Gaudence IRADUKUNDA**\n",
    "- **Stephen TAIWO**   \n",
    "- **Emile NIYITANGA**  \n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:purple;\">**Institution:**</span> African Institute for Mathematical Sciences \n",
    "### <span style=\"color:purple;\">**Supervisor:**</span> Olakunle S. Abawonse  \n",
    "### <span style=\"color:purple;\">**Date:**</span> 25 January 2025\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "------------------------------------\n",
    "## SCOPE OF THE PROJECT\n",
    "\n",
    "In this project, we will be working on an audio dataset, which consists of normal breathinga and asthmatic breathing. We want classify the audios into their respective classes by converting each audio to a signal, and extracting topological features using different methods. These methods include Persistent Entropy, various metrics such as bottleneck, heat, and Betti numbers, as well as Carlsson Coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98857a26-5a57-4f14-bec9-49197cb338d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary packages and modules\n",
    "\n",
    "# data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import YouTubeVideo\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "# hepml\n",
    "from hepml.core import make_gravitational_waves, download_dataset\n",
    "\n",
    "# tda magic\n",
    "from gtda.homology import VietorisRipsPersistence, CubicalPersistence\n",
    "from gtda.diagrams import PersistenceEntropy, Scaler, NumberOfPoints, Amplitude\n",
    "from gtda.plotting import plot_heatmap, plot_point_cloud, plot_diagram\n",
    "from gtda.pipeline import Pipeline\n",
    "from gtda.time_series import TakensEmbedding, SingleTakensEmbedding\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from teaspoon.ML import feature_functions as Ff\n",
    "\n",
    "# ml tools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "\n",
    "# dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "import sys\n",
    "sns.set(color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-shield",
   "metadata": {},
   "source": [
    "Here, we try to organize our data set into training and test. To achieve this, about 80% of the audio files from each category (Normal and Asthma) are allocated to the training set, while the remaining from each category is used for the test set. The training and test data, along with their respective labels, are then saved into separate `.csv` files for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33474f05-151d-48d1-949a-9f77519b5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "normal_folder = '/home/stephen/Downloads/TDA/Project_3/Normal'\n",
    "asthma_folder = '/home/stephen/Downloads/TDA/Project_3/Asthma/'\n",
    "\n",
    "normal_files = [os.path.join(normal_folder, f) for f in os.listdir(normal_folder) if f.endswith(\".wav\")]\n",
    "asthma_files = [os.path.join(asthma_folder, f) for f in os.listdir(asthma_folder) if f.endswith(\".wav\")]\n",
    "\n",
    "# Randomly select files for training and test sets\n",
    "normal_train = random.sample(normal_files, 73)\n",
    "normal_test = random.sample([f for f in normal_files if f not in normal_train], 32)\n",
    "\n",
    "asthma_train = random.sample(asthma_files, 67)\n",
    "asthma_test = random.sample([f for f in asthma_files if f not in asthma_train], 28)\n",
    "\n",
    "# Putting all file names and labels in a dataframe\n",
    "data1 = {\n",
    "    \"file_path\": normal_train + asthma_train,\n",
    "    \"label\": [0] * len(normal_train) + [1] * len(asthma_train)}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "\n",
    "df1.to_csv(\"train_data.csv\", index=False)\n",
    "\n",
    "data2 = {\n",
    "    \"file_path\": normal_test + asthma_test,\n",
    "    \"label\": [0] * len(normal_test) + [1] * len(asthma_test)}\n",
    "\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "\n",
    "df2.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-swift",
   "metadata": {},
   "source": [
    "### Functions to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4da040-a317-4842-8dba-6eba8f810b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def convert_dgm(dgm):\n",
    "    Arr = dgm.copy()\n",
    "    Arr[0] = Arr[0][:-1]\n",
    "    col_a  = np.zeros(Arr[0].shape[0])\n",
    "    Arr[0] = np.column_stack((Arr[0], col_a))\n",
    "    \n",
    "    col_b  = np.ones(Arr[1].shape[0], dtype=int)\n",
    "    Arr[1] = np.column_stack((Arr[1], col_b))\n",
    "    temp_1 = list(Arr[0])\n",
    "    temp_2 = list(Arr[1])\n",
    "    temp_1.extend(temp_2)\n",
    "    return np.asarray(temp_1)\n",
    "\n",
    "def fit_embedder(embedder, y, verbose=True):\n",
    "    y_embedded = embedder.fit_transform(y)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Shape of embedded time series: {y_embedded.shape}\")\n",
    "        print(f\"Optimal embedding dimension is {embedder.dimension_} and time delay is {embedder.time_delay_}\")\n",
    "\n",
    "    return y_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e9976d-9c72-4efc-a42d-8ddc12b5754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file):\n",
    "    \n",
    "    spf = wave.open(file, \"r\")\n",
    "    signal = spf.readframes(-1)\n",
    "    signal = np.frombuffer(signal, np.int16)\n",
    "    \n",
    "    # Takens Embedding parameters\n",
    "    embedding_dimension = 30\n",
    "    embedding_time_delay = 100\n",
    "    stride = 15\n",
    "\n",
    "    # Create a SingleTakensEmbedding object\n",
    "    embedder = SingleTakensEmbedding(\n",
    "        parameters_type=\"search\", n_jobs=2, time_delay=embedding_time_delay, dimension=embedding_dimension, stride=stride\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the signal\n",
    "    y_noise_embedded = embedder.fit_transform(signal)\n",
    "\n",
    "    # Compute persistent homology using Ripser\n",
    "    res = ripser(y_noise_embedded, n_perm=700)\n",
    "    dgms_sub = res['dgms']\n",
    "\n",
    "    # Convert diagrams (assuming this is a custom function)\n",
    "    res = convert_dgm(dgms_sub)\n",
    "\n",
    "    # Extract persistence diagrams\n",
    "    test = dgms_sub[0][:-1]  # H0\n",
    "    test_1 = dgms_sub[1]     # H1\n",
    "\n",
    "    # Compute feature matrix using custom function Ff.F_CCoordinates\n",
    "    FN = 5\n",
    "    FeatureMatrix, TotalNumComb, CombList = Ff.F_CCoordinates(test[None, :, :], FN)\n",
    "    X_cc_0 = FeatureMatrix[-4]\n",
    "    \n",
    "    FeatureMatrix, TotalNumComb, CombList = Ff.F_CCoordinates(test_1[None, :, :], FN)\n",
    "    X_cc_1 = FeatureMatrix[-3]\n",
    "\n",
    "    # Define metrics for additional features\n",
    "    metrics = [\n",
    "        {\"metric\": \"bottleneck\", \"metric_params\": {}},\n",
    "        {\"metric\": \"wasserstein\", \"metric_params\": {\"p\": 2}},\n",
    "        {\"metric\": \"betti\", \"metric_params\": {\"p\": 2, \"n_bins\": 100}},\n",
    "        {\"metric\": \"landscape\", \"metric_params\": {\"p\": 2, \"n_layers\": 2, \"n_bins\": 100}},\n",
    "        {\"metric\": \"heat\", \"metric_params\": {\"p\": 2, \"sigma\": 1.6, \"n_bins\": 100}},\n",
    "        {\"metric\": \"heat\", \"metric_params\": {\"p\": 2, \"sigma\": 3.2, \"n_bins\": 100}},\n",
    "    ]\n",
    "\n",
    "    # Create a feature union with persistence diagram metrics\n",
    "    feature_union = make_union(\n",
    "        PersistenceEntropy(normalize=True),\n",
    "        NumberOfPoints(n_jobs=-1),\n",
    "        *[Amplitude(**metric, n_jobs=-1) for metric in metrics]\n",
    "    )\n",
    "\n",
    "    # Fit and transform persistence diagrams\n",
    "    single_data = feature_union.fit_transform(res[None, :, :])\n",
    "    X_metrics = single_data\n",
    "\n",
    "    # Concatenate all features\n",
    "    single_X_train = np.concatenate((X_cc_0, X_cc_1, X_metrics), axis=None)\n",
    "\n",
    "    return single_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319d31f4-8f99-4926-bcfb-331640ec2486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the extraction function on a single file\n",
    "test_file = \"/home/stephen/Downloads/TDA/Project_3/Normal/BP30_N,N,P R M,18,F.wav\"\n",
    "features = extract_features(test_file)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-consistency",
   "metadata": {},
   "source": [
    "### Extracting Features\n",
    "\n",
    "To extract the features from the signal, we use Carlsson Coordinates(8), Persistence Entropy(2), Number of Points(2) and Amplitude(12) with different metrics: bottleneck, wasserstein, betti, landscape, and heat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "married-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files for the training and test sets\n",
    "train_df = pd.read_csv(\"train_data.csv\") \n",
    "test_df = pd.read_csv(\"test_data.csv\")    \n",
    "\n",
    "X_train, y_train = [], []\n",
    "X_test, y_test = [], []\n",
    "\n",
    "# Extract features for the training set\n",
    "for index, row in train_df.iterrows():\n",
    "    file_path = row['file_path']\n",
    "    label = row['label']\n",
    "    \n",
    "    try:\n",
    "        feature = extract_features(file_path)\n",
    "        X_train.append(feature)\n",
    "        y_train.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "marked-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for the test set\n",
    "for index, row in test_df.iterrows():\n",
    "    file_path = row['file_path']\n",
    "    label = row['label']\n",
    "    \n",
    "    try:\n",
    "        feature = extract_features(file_path)\n",
    "        X_test.append(feature)\n",
    "        y_test.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d2672d-9c19-4ffe-a811-725d1e0dc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "improved-thriller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accurate-consultancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-luxembourg",
   "metadata": {},
   "source": [
    "We obtained 24 features in total. This features will be used to train and classifier and see its performance.\n",
    "\n",
    "### Training a Classifier\n",
    "\n",
    "Let's train a Random Forest Classifier to see how the features help us classify the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fallen-opportunity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 31)\n",
    "rf.fit(X_train , y_train)\n",
    "\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pharmaceutical-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(fitted_model):\n",
    "    res = {\n",
    "        \"Accuracy on train:\": accuracy_score(fitted_model.predict(X_train), y_train),\n",
    "        \"ROC AUC on train:\": roc_auc_score(y_train, fitted_model.predict_proba(X_train)[:, 1]),\n",
    "        \"Accuracy on valid:\": accuracy_score(fitted_model.predict(X_test), y_test),\n",
    "        \"ROC AUC on valid:\": roc_auc_score(y_test, fitted_model.predict_proba(X_test)[:, 1]),\n",
    "    }\n",
    "    if hasattr(fitted_model, \"oob_score_\"):\n",
    "        res[\"OOB accuracy:\"] = fitted_model.oob_score_\n",
    "\n",
    "    for k, v in res.items():\n",
    "        print(k, round(v, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "naked-preparation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.993\n",
      "ROC AUC on train: 1.0\n",
      "Accuracy on valid: 0.65\n",
      "ROC AUC on valid: 0.624\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=31)\n",
    "rf.fit(X_train, y_train)\n",
    "print_scores(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-somerset",
   "metadata": {},
   "source": [
    "The accuracy of the classifier is observed to be 65% indicating that it performs moderately well but still leaves room for improvement. This can be improved by looking carefully into our feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
